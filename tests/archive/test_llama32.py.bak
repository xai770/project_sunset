#!/usr/bin/env python3
"""
Test script to run the pipeline with only llama3.2 generation for specific jobs using a custom prompt.
"""
import sys
import os
import argparse
from pathlib import Path

# Get the project root directory
PROJECT_ROOT = Path(__file__).parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from run_pipeline.core.cli_args import parse_args
from run_pipeline.config.paths import JOB_DATA_DIR
import json

# Custom prompt for llama3.2 (from profile/cv/2025-05-23 llama3.2 prompt role2cv.md)
LLAMA3_PROMPT = '''Please read section 1 and follow the instructions.
# 1. Instructions
!!! Do not generate any output, until you reach section 1.2. !!!

##  1.1. Analysis and preparation
You are an expert job application assistant.
Review the CV in section 2.1 and the role description in section 2.2. They contain all the information you need.

1.1.1. Compare the role requirements carefully with the evidence in the CV. Based on your careful comparison, determine the CV-to-role match level: 
    - **Low match:** if the CV does not state direct experience  in any key requirement  of the role title and details (e.g., specific technology, industry, or critical skill explicitly required). THIS RULE IS ALWAYS VALID!
    - **Moderate match:**  if I have gaps in secondary requirements. 
    - **Good match:** if I have only minor gaps and decide to apply.
1.1.2. Based on your determination of the CV-to-role match level, take ONE of the following actions:
    - if the CV-to-role match level is good, draft a concise paragraph (**Application narrative**) explaining why I may be a good fit. Do NOT draft a full cover letter - only one paragraph.
    - in all other cases, i.e. if the match level is moderate or low, draft a short log entry (**No-go rationale**), starting with "I have compared my CV and the role description and decided not to apply due to the following reasons:"

## 1.2. Generate output
Output ONLY the elements below. Do NOT add anything, as this will generate an error.
    
**CV-to-role match:** [Low match/Moderate match/Good match]
**Application narrative:** [if the CV-to-role match level is good]
**No-go rationale:** [in all other cases, i.e. if the match level is moderate or low]]

# 2. Input

## 2.1. CV:
{cv}

## 2.2. Role Description:
{job}
'''

# Use the same LLM call as phi3, but with model="llama3.2:latest"
def call_llama3_api(prompt):
    from run_pipeline.utils.llm_client import call_ollama_api
    return call_ollama_api(prompt, model="llama3.2:latest")

def get_cv_markdown_text():
    """Get the CV text from the Markdown file."""
    cv_path = os.path.join(PROJECT_ROOT, "profile", "cv", "Gershons_concise_cv.md")
    try:
        with open(cv_path, "r", encoding="utf-8") as cf:
            return cf.read()
    except FileNotFoundError:
        print(f"CV file not found at {cv_path}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="Run llama3.2 generation for specific jobs")
    parser.add_argument("--job-id", default="61691", help="Job ID to process (defaults to 61691 if not provided)")
    parser.add_argument("--force", action="store_true", help="Force reprocessing even if fields already exist")
    parser.add_argument("--dump-input", action="store_true", help="Dump the LLM input to a text file and exit")
    our_args = parser.parse_args()
    job_id = our_args.job_id
    dump_input = our_args.dump_input
    print(f"Running pipeline with llama3.2 on job ID {job_id}")

    # Remove --dump-input from sys.argv before calling parse_args()
    filtered_argv = [arg for arg in sys.argv if arg != '--dump-input']
    sys.argv = filtered_argv
    args = parse_args()  # Start with default args
    args.job_ids = job_id
    args.only_regenerate_phi3 = True
    args.skip_fetch = True
    args.skip_status_check = True
    args.skip_skills = True
    args.force_reprocess = True

    # Prepare LLM input
    job_path = os.path.join(JOB_DATA_DIR, f"job{job_id}.json")
    with open(job_path, "r", encoding="utf-8") as jf:
        job_data = json.load(jf)
    web_details = job_data.get("web_details")
    if not web_details:
        print(f"No web_details found in {job_path}")
        return 1
    job_title = web_details.get("position_title", "")
    concise_desc = web_details.get("concise_description", "")
    job_description = f"Position Title: {job_title}\n\n{concise_desc}"
    cv_text = get_cv_markdown_text()
    prompt = LLAMA3_PROMPT.format(cv=cv_text, job=job_description)

    # If --dump-input is set, dump the LLM input and continue to call the LLM
    if dump_input:
        dump_path = os.path.join(os.path.dirname(job_path), f"job{job_id}_llm_input.txt")
        with open(dump_path, "w", encoding="utf-8") as outf:
            # First include the full prompt (which already contains the CV and job details)
            # Then add the job information after the separator for easier reference
            outf.write(f"PROMPT SENT TO LLM:\n\n{prompt}\n\n---\n\njob_title: {job_title}\n\nconcise_desc:\n{concise_desc}\n\njob_description:\n{job_description}\n")
        print(f"LLM input dumped to {dump_path}")
        # Do not return here; continue to call the LLM and save output

    # Call LLM and print result
    print("Calling llama3.2:latest LLM...")
    response = call_llama3_api(prompt)
    print("\n--- LLM RESPONSE ---\n")
    print(response)
    # Optionally, save the response to a file
    out_path = os.path.join(os.path.dirname(job_path), f"job{job_id}_llm_output.txt")
    with open(out_path, "w", encoding="utf-8") as outf:
        outf.write(response)
    print(f"\nLLM output saved to {out_path}")
    return 0

if __name__ == "__main__":
    sys.exit(main())

    # If --dump-input is set, dump the LLM input and continue to call the LLM
    if dump_input:
        dump_path = os.path.join(os.path.dirname(job_path), f"job{job_id}_llm_input.txt")
        with open(dump_path, "w", encoding="utf-8") as outf:
            # First include the full prompt (which already contains the CV and job details)
            # Then add the job information after the separator for easier reference
            outf.write(f"PROMPT SENT TO LLM:\n\n{prompt}\n\n---\n\njob_title: {job_title}\n\nconcise_desc:\n{concise_desc}\n\njob_description:\n{job_description}\n")
        print(f"LLM input dumped to {dump_path}")
        # Do not return here; continue to call the LLM and save output

    # Call LLM and print result
    print("Calling llama3.2:latest LLM...")
    response = call_llama3_api(prompt)
    print("\n--- LLM RESPONSE ---\n")
    print(response)
    # Optionally, save the response to a file
    out_path = os.path.join(os.path.dirname(job_path), f"job{job_id}_llm_output.txt")
    with open(out_path, "w", encoding="utf-8") as outf:
        outf.write(response)
    print(f"\nLLM output saved to {out_path}")
    return 0

if __name__ == "__main__":
    sys.exit(main())
